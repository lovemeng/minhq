__Issues with QCRAM__

I have implemented QCRAM and have a few things to comment on.

My implementation is here:
  https://github.com/martinthomson/minhq/tree/master/hc

This is my first attempt at go, so this took a little longer than it might have
otherwise. Test coverage is still a little light, though go thinks that I have
pretty good coverage. There's probably still a lot to go on the corner cases,
but I think that I've done enough to get an idea for the complexity bump.

I started by implementing HPACK, which is surprisingly easy if you don't care
about being efficient. Though I knew that I was going to add QCRAM, I made only
minimal design concessions when building HPACK.

Building QCRAM on top of HPACK took at least 50% more than just doing HPACK (at
a guess, HPACK took about 10-12 hours, QCRAM took closer to 20). Even though
you can reuse Huffman coding and most of the primitives, retrofitting it was
tricky. My lack of familiarity with go definitely contributed a lot to that,
but I'd say that QCRAM is more difficult to implement than it seems in theory.

I didn't implement QCRAM entirely as specified. There are a few things that
I'll describe below that would make it either better or easier to implement (or
both).


# Separate Stream Roles

There are actually three streams of data that are of interest, and there is no
clarity about those streams. 

1. I suggest that we use unidirectional streams in each direction for table
updates.

2. A separate stream for acknowledgments ensures that the encoder can consume
that separately to the decoder.

3. Request/response streams. I think that we all understand these.

These can be separate from the hq control stream, and they should be. Rather
than pick a specific stream number, we should add a header for these streams
that will allow them to be identified clearly. If we do that, add strict rules
about cardinality.

# Define Framing More Clearly

The definition of the framing that is used on the control stream is unclear. I
assume that the intent is to use HEADERS frames, but I'm only really guessing.
If you take my suggestion for separate framing, then you can use very different
logic for the different streams.

Honestly, the overhead of framing is excessive for anything but
request/response streams. I found that framing on the control stream wasn't
necessary at all. This might depend on implementation strategy, because it
might be easier to collect a framed thing than it is to read until you are
blocked, then pickle that state so that you can go back to something else. In
my implementation I hold state on the stack, so it's pretty easy.

Also, it should suffice to have just a stream of varints indicating
acknowledgments. Duplicates can be used to acknowledge multiple header blocks
on the same stream in order.

# Split the Instruction Space

Having to check that only certain instructions appear on different streams is
annoying to implement and a waste of bits. Define two separate spaces. That
should make things much simpler: there are exactly two instructions in each
space then (leaving aside acknowledgments, which I will cover below).

# Base in Every HEADERS Frame

Base (the number of inserts in the table at the start of a header block) seems
to be required for all HEADERS frames, including both table modification and
request/response streams.  But...

# Base on the Table Update Stream

You don't need base for table updates. It is more of a hazard than a help. It
is easier if you treat indexing on the table update stream as in HPACK where
each insert moves the reference point for the next insertion.

# Remove BLOCKING

The BLOCKING flag complicates the design unnecessarily. As a fixed overhead,
pay the extra byte always. Furthermore, the table update stream will never need
to use depends, so that further motivates having different instruction sets
(and formats) for different streams.  And you don't need it...

# Base and the Static Table

It is not explained at all that the base is applied to the dynamic table only.
That is, index 1 is a straight index into the static table and that index 62
will use the current number of inserts and the base to determine the actual
index into the dynamic table (this is the first table entry only when the
number of table inserts and base are the same, if the base is smaller, then it
will be further along the table).

To be clear, my logic for finding entries is:

    if i == 0: error
    if i <= len(staticTable): return staticTable[i-1]
    delta = tableBase - base
    if delta < 0: error
    return dynamicTable[i - len(staticTable) - 1 + delta]

# Remove Depends

Depends is useless. I did not implement it. Base is sufficient.

This follows from preferring to use the smallest value for base possible. Doing
that also keeps table references as small as possible. (Because references are
counted in the opposite direction to base, a larger base means larger
references.)

The cost is that you have to do a two pass check: the first to find the newest
reference, the second to write out the header block. I find that it's basically
necessary to do two passes anyway: on the first I write out the table updates,
on the second I write the header block. During that phase you want to do
lookups anyway, so you can save the largest base from that pass. Also, when you
insert anything, that tends to set the base, so it's pretty easy overall.

# Tracking ACKs is Harder Than It Seems

There are multiple header blocks per stream, and ACKs only identify the stream.
You can't just store a stream reference (or Request-ID) against each header
block, you have to store a tuple of stream and header block index. This needs
to be explained somewhere.

I decided to externalize ACKs - the QCRAM encoder takes a generic blob as input
when it encodes a header block and the same generic blob to acknowledge it. The
caller then has to combine stream ID and header block index in a sensible
fashion.

# Acknowledge Dmitry

Dmitry did a lot of work on the design team.

# Dynamic Table Overhead

The draft has a TODO about overhead for each dynamic entry. I added a field to
every entry that allows it to know its own position in the table, but that is a
simplification/optimization, and you don't really need that. The other
per-record overheads are all the same for the decoder, with a small increase to
the fixed overhead. It's a little more state for an encoder, but that is mostly
because of having to track usage.

I don't think that we need to change from a 32 byte overhead. This decision
could reduce code complexity in shared QCRAM/HPACK implementations (in my
implementation it would).

# Duplication Doesn't Need to Reference the Static Table

Duplication only makes sense for dynamic entries and yet the index is (I
assume), the normal table index. That is highly suboptimal. This is worse
because the prefix is only 5 bits for this instruction (up to 62 values), which
leads to the encoding taking an extra byte always. If you take my suggestion
for separate instruction spaces, this is better, but still not as good as a
direct index into the dynamic table, starting at 0. Better yet would be a
reverse index, starting from the end of the table, because duplication often
needs to refer to the end of the table.

# Need Better Advice on Referencing strategy

Referencing entries near the end of the table in a new header block is
hazardous. If you just compress as you do in HPACK, you can end up pinning
entries in the header table, making it impossible to add new entries. Thus, you
have to stop referencing entries as they get nearer to the table capacity
limit.

In my implementation I added a threshold variable that governs my referencing
strategy. A portion of the header table space is marked as being off-limits for
header block usage. The encoder duplicates those entries if it wants to
compress, but won't reference them. If the table is under pressure from the new
header fields being added, then it should be able to advance by at least this
amount every round trip.

This is a simple strategy, and probably suboptimal in various ways, but it
seems to work out. For instance, I continuously update the cutoff as entries
are inserted, so that a header block might end up referencing things that get
pushed into this space by later header fields.

You can imagine more advanced techniques. For instance, you might tune this
window automatically based on the rate that the header table grows. Because
references in that window block eviction, it should be sized to the rate of
*desired* growth in the header table over a round trip. Taking a running
estimate of that desired growth and allowing a margin for variance could
produce the right value. I haven't experimented with this.
